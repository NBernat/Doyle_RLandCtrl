Experiment descriptions.txt

Spinup_tests:
	Initial experiments to verify the functioning of spinup algorithms on standard openAI gym environments


Cdsalpha_tests:
	Tests verifying the function of the custom gym environment CdsAlpha, as well as initial tests probing RL performance on CdsAlpha. Alpha is a parameter that renders the system uncontrollable at/near zero, but as it increases, renders the system more and more unstable. Various amounts of noise were tested along a sweep of the parameter alpha. 
In this version of the environment, experiments are run on randomized initial states, and the stopping condition is a function of the state (these two properties were changed in the next version). 
	We saw definite trends in these tests-- that alpha~0 performs poorly (high error and variance), and that more noise => worse converged performance for good alpha (good was around 0.4-0.5), and that more unstable systems took longer to converge. We decided to fix the horizon length of the experiment because we want a more direct comparison with the LQR  methods we're testing on this system (thus finhoriz tests below).

Finhoriz_tests:
	In the finite horizon tests, the CdsAlpha environment was edited to run for a fixed number of steps per run in order to make comparisons with SysID/Optimal Control more natural. Also, for all experiments that have noise, the starting state is zero.
	We ran for a horizon length of 5. The scale of error is much smaller, since the trajectories are much shorter. I suspect that none of these would actually do well on long horizons, and that they are not getting sufficient signal out of the trajectories, because they seem to all converge very quickly, even in alpha=0 experiments; it seems like they all realize that doing nothing won't be penalized very heavily (would need to verify that the control input chosen is near zero), maybe because the regularization term is now dominating the reward.
	We are currently running a small set of tests for a much lower regularization parameter to see if the trends look more similar to the un-capped horizon tests.


(Future: kout_tests):
	These tests will be run in a manner similar to finhoriz tests, but the policy will output a linear controller instead of the actual control output. This is a 99% model-free method, and essentially gives the policy the additional knowledge that the optimal controller is linear. We expect that it should perform better than finhoriz. 


(Future: scalar_tests):
These tests will probe RL performance on a 1D-state system, where alpha only affects stability. This is meant to show how stability/instability affects performance in the absence of controlability issues.